# ü§ñ Jarvis AI Assistant

A personal AI assistant powered by a **self-hosted LLM (LLaMA)** with **Pinecone vector database** for knowledge retrieval (RAG) and a **Streamlit chatbot UI**.

## üèóÔ∏è Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                        Jarvis AI Assistant                       ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                  ‚îÇ
‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ   ‚îÇ   Streamlit  ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   FastAPI    ‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∂‚îÇ   Ollama     ‚îÇ   ‚îÇ
‚îÇ   ‚îÇ   Chat UI    ‚îÇ     ‚îÇ   Backend    ‚îÇ     ‚îÇ   (LLaMA)    ‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ                               ‚îÇ                                  ‚îÇ
‚îÇ                               ‚ñº                                  ‚îÇ
‚îÇ                        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                         ‚îÇ
‚îÇ                        ‚îÇ   Pinecone   ‚îÇ                         ‚îÇ
‚îÇ                        ‚îÇ Vector Store ‚îÇ                         ‚îÇ
‚îÇ                        ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                         ‚îÇ
‚îÇ                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìã Features

- **Self-hosted LLM**: Uses Ollama with LLaMA model for privacy-first AI
- **RAG (Retrieval Augmented Generation)**: Contextual responses using your knowledge base
- **Vector Database**: Pinecone for efficient semantic search
- **Conversational Memory**: Maintains chat context across messages
- **Knowledge Management**: Add, search, and delete documents from UI
- **Streaming Responses**: Real-time response generation
- **REST API**: Full API for programmatic access

## üöÄ Quick Start

### Prerequisites

1. **Python 3.9+**
2. **Ollama** - Self-hosted LLM runtime
3. **Pinecone Account** - Free tier available

### 1. Install Ollama

```bash
# Linux
curl -fsSL https://ollama.com/install.sh | sh

# macOS
brew install ollama

# Start Ollama service
ollama serve

# Pull LLaMA model
ollama pull llama2
```

### 2. Set Up Pinecone

1. Create account at [pinecone.io](https://www.pinecone.io/)
2. Create a new project
3. Get your API key from the dashboard

### 3. Configure Environment

```bash
# Clone and enter directory
cd jarvis

# Copy environment template
cp .env.example .env

# Edit .env with your credentials
nano .env
```

Update `.env` with your values:
```env
PINECONE_API_KEY=your-actual-api-key
PINECONE_INDEX_NAME=jarvis-knowledge
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama2
```

### 4. Install Dependencies

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # Linux/macOS
# or: venv\Scripts\activate  # Windows

# Install packages
pip install -r requirements.txt
```

### 5. Run the Application

```bash
# Terminal 1: Start the API backend
python main.py

# Terminal 2: Start the Streamlit UI
streamlit run app.py
```

The application will be available at:
- **Chat UI**: http://localhost:8501
- **API Docs**: http://localhost:8000/docs

## üìö API Reference

### Chat Endpoints

#### POST `/chat`
Send a message and get a response.

```json
{
  "message": "What is our company policy on remote work?",
  "conversation_history": [],
  "use_knowledge_base": true
}
```

#### POST `/chat/stream`
Stream a response in real-time.

### Knowledge Management

#### POST `/knowledge/add`
Add a document to the knowledge base.

```json
{
  "doc_id": "remote_policy",
  "content": "Our company allows remote work up to 3 days per week...",
  "metadata": {"category": "HR", "version": "2024"}
}
```

#### POST `/knowledge/search`
Search the knowledge base.

```json
{
  "query": "remote work policy",
  "top_k": 5
}
```

#### DELETE `/knowledge/{doc_id}`
Delete a document from the knowledge base.

## üìÅ Project Structure

```
jarvis/
‚îú‚îÄ‚îÄ main.py              # FastAPI backend server
‚îú‚îÄ‚îÄ app.py               # Streamlit chat UI
‚îú‚îÄ‚îÄ config.py            # Configuration management
‚îú‚îÄ‚îÄ models.py            # Pydantic models
‚îú‚îÄ‚îÄ ingest.py            # Knowledge ingestion utilities
‚îú‚îÄ‚îÄ requirements.txt     # Python dependencies
‚îú‚îÄ‚îÄ .env.example         # Environment template
‚îú‚îÄ‚îÄ .env                 # Your configuration (create this)
‚îî‚îÄ‚îÄ services/
    ‚îú‚îÄ‚îÄ __init__.py
    ‚îú‚îÄ‚îÄ llm_service.py   # Ollama/LLaMA integration
    ‚îî‚îÄ‚îÄ vector_db.py     # Pinecone integration
```

## üîß Ingesting Knowledge

### From Python

```python
from ingest import ingest_text, ingest_file, ingest_directory

# Add text directly
ingest_text(
    text="Company vacation policy allows 20 days PTO per year...",
    doc_id="vacation_policy",
    metadata={"category": "HR"}
)

# Ingest a file
ingest_file("documents/handbook.txt")

# Ingest entire directory
ingest_directory("documents/", extensions=[".txt", ".md"], recursive=True)
```

### From Command Line

```bash
# Ingest a single file
python ingest.py documents/policy.txt

# Ingest a directory recursively
python ingest.py documents/ -r -e .txt .md
```

## üõ†Ô∏è Customization

### Using Different LLM Models

```bash
# Pull a different model
ollama pull mistral
ollama pull codellama

# Update .env
OLLAMA_MODEL=mistral
```

### Adjusting Embedding Model

Edit `config.py`:
```python
EMBEDDING_MODEL = "all-mpnet-base-v2"  # Larger, more accurate
EMBEDDING_DIMENSION = 768
```

> **Note**: If you change embedding dimensions, you'll need to recreate your Pinecone index.

## üêõ Troubleshooting

### Ollama Connection Error
```
Error communicating with LLM service
```
- Ensure Ollama is running: `ollama serve`
- Check if model is downloaded: `ollama list`

### Pinecone Authentication Error
- Verify your API key in `.env`
- Check if you're using the correct environment

### Slow Responses
- Consider using a smaller model: `ollama pull llama2:7b`
- Reduce `top_k` for knowledge retrieval

## üìÑ License

MIT License - Feel free to use and modify for your enterprise needs!

---

Built with ‚ù§Ô∏è for the "Code Meets Co-Pilot" workshop
